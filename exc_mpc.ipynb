{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exc_mpc.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ljXfMD_jQ82y",
        "yG7FZ7RRYGC-"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeDVUFpMyzrQ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3XyQxAXy-w0"
      },
      "source": [
        "!pip install pybullet stable-baselines3[extra] motion-imitation==0.0.2 requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5XFhr7DLKRO"
      },
      "source": [
        "# import a1 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQtaaN1NzHsf"
      },
      "source": [
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/xiaoa5/robotics_control/main/a1_sim.py\"\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('a1_sim.py','w') as f:\n",
        "  f.write(r.text)\n",
        "# from google.colab import files\n",
        "# # files.upload()\n",
        "# src = list(files.upload().values())[0]\n",
        "# open('a1_sim.py','wb').write(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljXfMD_jQ82y"
      },
      "source": [
        "# gym style enviorment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ1hY8M5Q3rT"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "#from __future__ import google_type_annotations\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import inspect\n",
        "\n",
        "from absl import app\n",
        "from absl import flags\n",
        "import scipy.interpolate\n",
        "import numpy as np\n",
        "import pybullet_data as pd\n",
        "from pybullet_utils import bullet_client\n",
        "\n",
        "import time\n",
        "import pybullet\n",
        "import random\n",
        "\n",
        "from mpc_controller import com_velocity_estimator\n",
        "from mpc_controller import gait_generator as gait_generator_lib\n",
        "from mpc_controller import locomotion_controller\n",
        "from mpc_controller import openloop_gait_generator\n",
        "from mpc_controller import raibert_swing_leg_controller\n",
        "from mpc_controller import torque_stance_leg_controller\n",
        "from google.colab import files\n",
        "# files.upload()\n",
        "# src = list(files.upload().values())[0]\n",
        "# open('a1_sim.py','wb').write(src)\n",
        "import a1_sim as laikago_sim\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "_NUM_SIMULATION_ITERATION_STEPS = 300\n",
        "_BODY_HEIGHT = 0.24\n",
        "# _STANCE_DURATION_SECONDS = [\n",
        "#     0.3\n",
        "# ] * 4  # For faster trotting (v > 1.5 ms reduce this to 0.13s).\n",
        "# _DUTY_FACTOR = [0.6] * 4\n",
        "\n",
        "_STANCE_DURATION_SECONDS = [0.15]*4   #(0.3, 0.3, 0.3, 0.3)\n",
        "_DUTY_FACTOR = [0.6]*4\n",
        "_INIT_PHASE_FULL_CYCLE = [0., 0.25, 0.5, 0.25]\n",
        "_MAX_TIME_SECONDS = 250\n",
        "_MOTOR_KD = [1.0, 2.0, 2.0] * 4\n",
        "\n",
        "LAIKAGO_THREE = (\n",
        "    gait_generator_lib.LegState.STANCE,\n",
        "    gait_generator_lib.LegState.STANCE,\n",
        "    gait_generator_lib.LegState.STANCE,\n",
        "    gait_generator_lib.LegState.SWING,\n",
        ")\n",
        "LAIKAGO_TROTTING = (\n",
        "    gait_generator_lib.LegState.SWING,\n",
        "    gait_generator_lib.LegState.STANCE,\n",
        "    gait_generator_lib.LegState.STANCE,\n",
        "    gait_generator_lib.LegState.SWING,\n",
        ")\n",
        "def _generate_example_linear_angular_speed(t):\n",
        "  \"\"\"Creates an example speed profile based on time for demo purpose.\"\"\"\n",
        "  vx = 0.6\n",
        "  vy = 0.2\n",
        "  wz = 0.8\n",
        "  time_points = (0, 5, 10, 15, 20, 25,30)\n",
        "  speed_points = ((0, 0, 0, 0), (0, 0, 0, wz), (vx, 0, 0, 0), (0, 0, 0, -wz), (0, -vy, 0, 0),\n",
        "                  (0, 0, 0, 0), (0, 0, 0, wz))\n",
        "\n",
        "  speed = scipy.interpolate.interp1d(\n",
        "      time_points,\n",
        "      speed_points,\n",
        "      kind=\"previous\",\n",
        "      fill_value=\"extrapolate\",\n",
        "      axis=0)(\n",
        "          t)\n",
        "\n",
        "  return speed[0:3], speed[3]\n",
        "\n",
        "\n",
        "def _setup_controller(robot):\n",
        "  \"\"\"Demonstrates how to create a locomotion controller.\"\"\"\n",
        "  desired_speed = (0, 0)\n",
        "  desired_twisting_speed = 0\n",
        "\n",
        "  gait_generator = openloop_gait_generator.OpenloopGaitGenerator(\n",
        "      robot,\n",
        "      stance_duration=_STANCE_DURATION_SECONDS,\n",
        "      duty_factor=_DUTY_FACTOR,\n",
        "      initial_leg_phase=_INIT_PHASE_FULL_CYCLE\n",
        "  )\n",
        "  state_estimator = com_velocity_estimator.COMVelocityEstimator(robot)\n",
        "  sw_controller = raibert_swing_leg_controller.RaibertSwingLegController(\n",
        "      robot,\n",
        "      gait_generator,\n",
        "      state_estimator,\n",
        "      desired_speed=desired_speed,\n",
        "      desired_twisting_speed=desired_twisting_speed,\n",
        "\n",
        "      desired_height=_BODY_HEIGHT,\n",
        "  )\n",
        "  st_controller = torque_stance_leg_controller.TorqueStanceLegController(\n",
        "      robot,\n",
        "      gait_generator,\n",
        "      state_estimator,\n",
        "      desired_speed=desired_speed,\n",
        "      desired_twisting_speed=desired_twisting_speed,\n",
        "\n",
        "\n",
        "      desired_body_height=_BODY_HEIGHT,\n",
        "      body_mass= 108 / 9.8,  #68 / 9.8,\n",
        "      # body_inertia=(0.07335, 0, 0, 0, 0.25068, 0, 0, 0, 0.25447),\n",
        "      body_inertia=(0.068, 0, 0, 0, 0.228, 0, 0, 0, 0.256),   #np.array((0.017, 0, 0, 0, 0.057, 0, 0, 0, 0.064)) * 0.1  #\n",
        "  )\n",
        "\n",
        "  controller = locomotion_controller.LocomotionController(\n",
        "      robot=robot,\n",
        "      gait_generator=gait_generator,\n",
        "      state_estimator=state_estimator,\n",
        "      swing_leg_controller=sw_controller,\n",
        "      stance_leg_controller=st_controller,\n",
        "      clock=robot.GetTimeSinceReset)\n",
        "  return controller\n",
        "\n",
        "\n",
        "def _update_controller_params(controller, lin_speed, ang_speed):\n",
        "  controller.swing_leg_controller.desired_speed = lin_speed\n",
        "  controller.swing_leg_controller.desired_twisting_speed = ang_speed\n",
        "  controller.stance_leg_controller.desired_speed = lin_speed\n",
        "  controller.stance_leg_controller.desired_twisting_speed = ang_speed\n",
        "\n",
        "from gym.utils import seeding\n",
        "import gym\n",
        "from gym import spaces\n",
        "class CustomGymEnv(gym.Env):\n",
        "\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 100}\n",
        "    def __init__(self, render=False):\n",
        "        # Set up logging.\n",
        "        self._is_render = render\n",
        "        if self._is_render:\n",
        "            self.p = bullet_client.BulletClient() #(connection_mode=pybullet.GUI)\n",
        "        else:\n",
        "            self.p = bullet_client.BulletClient()\n",
        "        # self.p = bullet_client.BulletClient(connection_mode=pybullet.GUI)\n",
        "        self.steering1Slider = self.p.addUserDebugParameter(\"forward  backward\", -0.6, 3.6, 0)\n",
        "        self.steering2Slider = self.p.addUserDebugParameter(\"left  right\", -0.2, 0.2, 0)\n",
        "        self.steering3Slider = self.p.addUserDebugParameter(\"turnLeft  tureRight\", -0.8, 0.8, 0)\n",
        "        self.forcexSlider = self.p.addUserDebugParameter(\"X Richtung\", -2.8, 2.8, 0)\n",
        "        self.forceySlider = self.p.addUserDebugParameter(\"Y Richtung\", -2.8, 2.8, 0)\n",
        "        self.forcezSlider = self.p.addUserDebugParameter(\"Z Richtung\", -2.8, 2.8, 0)\n",
        "        self.force1Slider = self.p.addUserDebugParameter(\"force applied on Dog\", 0, 3000, 0)\n",
        "        self.stanceDurationSlider = self.p.addUserDebugParameter(\"stance Duration\", 0.05, 0.5, 0.18)       \n",
        "        self.observation_space = spaces.Box(low=-20, high=20,shape=(37,), dtype=np.float32)  # 12 + 12 + 3 + 3 =30\n",
        "        self.action_space = spaces.Box(low=-0.3, high=0.3,shape=(12,), dtype=np.float32)\n",
        "        self.obs = None\n",
        "\n",
        "        self.seed()\n",
        "        self.reset()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.p.resetSimulation()\n",
        "        self.p.setAdditionalSearchPath(pd.getDataPath())\n",
        "        num_bullet_solver_iterations = int(_NUM_SIMULATION_ITERATION_STEPS /laikago_sim.ACTION_REPEAT)\n",
        "        self.p.setPhysicsEngineParameter(numSolverIterations=num_bullet_solver_iterations)\n",
        "        self.p.setTimeStep(laikago_sim.time_step)\n",
        "        self.p.setGravity(0, 0, -10)\n",
        "        self.p.setPhysicsEngineParameter(enableConeFriction=0)\n",
        "        planeShape = self.p.createCollisionShape(shapeType = self.p.GEOM_PLANE)\n",
        "        ground_id  = self.p.createMultiBody(0, planeShape)\n",
        "        self.p.resetBasePositionAndOrientation(ground_id,[0,0,0], [0,0,0,1])\n",
        "        self.p.changeVisualShape(ground_id, -1, rgbaColor=[0,0.7,0.5,0.8])\n",
        "        self.p.changeDynamics(ground_id, -1, lateralFriction=2.0)\n",
        "        deltaH = random.random()/20\n",
        "        self.robot_uid = self.p.loadURDF(\"a1/a1.urdf\", [0, 0, 0.28+deltaH])\n",
        "        self.robot = laikago_sim.SimpleRobot(self.p, self.robot_uid)\n",
        "        self.controller = _setup_controller(self.robot)\n",
        "        self.controller.reset()\n",
        "\n",
        "        self.current_time = self.robot.GetTimeSinceReset()\n",
        "        self.p.configureDebugVisualizer(self.p.COV_ENABLE_RENDERING, 1)\n",
        "        self._env_step_counter = 0\n",
        "        obs = self.mpc_action()\n",
        "        # self.obs = obs\n",
        "        return np.clip(obs,-20,20)\n",
        "\n",
        "    def is_fallen(self):\n",
        "        \n",
        "        pos, orientation = self.p.getBasePositionAndOrientation(self.robot.quadruped)\n",
        "        # rot_mat = self.p.getMatrixFromQuaternion(orientation)\n",
        "        # local_up = rot_mat[6:]\n",
        "        roll_pitch_yaw = self.p.getEulerFromQuaternion(orientation)\n",
        "\n",
        "        # print(np.dot(np.asarray([0, 0, 1]), np.asarray(local_up)))\n",
        "        # pos = self.robot.GetBasePosition()\n",
        "        # print(roll_pitch_yaw)\n",
        "        return  pos[2] < 0.135 or pos[2] > 0.8 #np.dot(np.asarray([0, 0, 1]), np.asarray(local_up)) < 0 and\n",
        "\n",
        "    def _termination(self):\n",
        "        # position = self.robot.GetTrueBaseOrientation()\n",
        "        # distance = math.sqrt(position[0]**2 + position[1]**2)\n",
        "        # print(self.is_fallen())\n",
        "        return self.is_fallen()  # or distance > self._distance_limit\n",
        "    \n",
        "    def get_obs(self,acts):\n",
        "        # acts = self.obs\n",
        "\n",
        "        obs0 = []\n",
        "        obs1 = []  # swing 0 stance 1\n",
        "        for i in range(12):\n",
        "            if acts[i*5] == 0 and acts[i*5 + 1] == 0:\n",
        "                obs0.extend([1])\n",
        "                obs1.extend([acts[i*5+4]])\n",
        "            else:\n",
        "                obs0.extend([0])\n",
        "                obs1.extend([acts[i*5]])\n",
        "\n",
        "        pos, orientation = self.p.getBasePositionAndOrientation(self.robot.quadruped)\n",
        "        posV, orientationV = self.p.getBaseVelocity(self.robot.quadruped)\n",
        "        # print(pos, orientation,posV, orientationV)\n",
        "        # print(acts)\n",
        "        # print(pos, orientation)\n",
        "        obs0.extend(obs1)\n",
        "        obs0.extend(pos)\n",
        "        obs0.extend(posV)\n",
        "        obs0.extend(orientation)\n",
        "        obs0.extend(orientationV)\n",
        "        # print(np.shape(obs0))\n",
        "        return obs0\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "    def mpc_action(self):\n",
        "        if self.current_time < 3000:\n",
        "            if self._is_render:\n",
        "\n",
        "                # steeringAngle1 = self.p.readUserDebugParameter(self.steering1Slider)\n",
        "                # steeringAngle2 = self.p.readUserDebugParameter(self.steering2Slider)\n",
        "                # steeringAngle3 = self.p.readUserDebugParameter(self.steering3Slider)\n",
        "\n",
        "                # forcex = self.p.readUserDebugParameter(self.forcexSlider)\n",
        "                # forcey = self.p.readUserDebugParameter(self.forceySlider)\n",
        "                # forcez = self.p.readUserDebugParameter(self.forcezSlider)\n",
        "\n",
        "                # force1 = self.p.readUserDebugParameter(self.force1Slider)\n",
        "                # stanceDuration1 = self.p.readUserDebugParameter(self.stanceDurationSlider)\n",
        "                # stanceDuration = [stanceDuration1] * 4 \n",
        "                pass\n",
        "            else:\n",
        "                steeringAngle1 = 3.6\n",
        "                steeringAngle2 = 0\n",
        "                steeringAngle3 = 0\n",
        "\n",
        "                forcex = 0\n",
        "                forcey = 0\n",
        "                forcez = 0\n",
        "\n",
        "                force1 = 0\n",
        "                stanceDuration1 = 0.18\n",
        "                stanceDuration = [stanceDuration1] * 4 \n",
        "\n",
        "            steeringAngle1 = 3.6\n",
        "            steeringAngle2 = 0\n",
        "            steeringAngle3 = 0\n",
        "\n",
        "            forcex = 0\n",
        "            forcey = 0\n",
        "            forcez = 0\n",
        "\n",
        "            force1 = 0\n",
        "            stanceDuration1 = 0.18\n",
        "            stanceDuration = [stanceDuration1] * 4 \n",
        "\n",
        "\n",
        "            lin_speed = [steeringAngle1,steeringAngle2,0]\n",
        "            ang_speed = steeringAngle3\n",
        "\n",
        "            rpy = (forcex,forcey,forcez)\n",
        "\n",
        "            start = time.time()\n",
        "    \n",
        "            _update_controller_params(self.controller, lin_speed, ang_speed)\n",
        "            # print(rpy,\"-----------------\")\n",
        "            \n",
        "            # Needed before every call to get_action().\n",
        "            self.controller.update()\n",
        "\n",
        "            start1 = time.time()\n",
        "\n",
        "            try:\n",
        "\n",
        "\n",
        "                hybrid_action = self.controller.get_action()\n",
        "            except:\n",
        "                hybrid_action = self.obs\n",
        "                print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
        "            obs0 = []\n",
        "            obs1 = []  # swing 0 stance 1\n",
        "            for i in range(12):\n",
        "                if hybrid_action[i*5] == 0 and hybrid_action[i*5 + 1] == 0:\n",
        "                    obs0.extend([1])\n",
        "                    obs1.extend([hybrid_action[i*5+4]])\n",
        "                else:\n",
        "                    obs0.extend([0])\n",
        "                    obs1.extend([hybrid_action[i*5]])\n",
        "\n",
        "            obs3 = self.robot.GetBaseRollPitchYawRate()                     # [-0.29003763 -0.15708871  0.03630358]\n",
        "            obs4 = self.controller.state_estimator.com_velocity_body_frame  #(-0.08136581629514694, -0.06520375609397888, 0.02040734514594078)\n",
        "            # print(obs4)  \n",
        "            out = obs1\n",
        "\n",
        "            end = time.time()\n",
        "\n",
        "            # self.render()\n",
        "\n",
        "            gemPos, gemOrn = self.p.getBasePositionAndOrientation(self.robot_uid)\n",
        "\n",
        "            self.p.resetDebugVisualizerCamera( cameraDistance=1.5, cameraYaw=-30, cameraPitch=-30, cameraTargetPosition=gemPos)\n",
        "            \n",
        "            force = force1*np.array([forcex,forcey,forcez])\n",
        "\n",
        "            \n",
        "            self.p.applyExternalForce(objectUniqueId=self.robot_uid, linkIndex=-1, forceObj=force\n",
        "                ,posObj=gemPos, flags=self.p.WORLD_FRAME)\n",
        "            # getExtendedObservation(robot_uid)\n",
        "            # time.sleep(0.001)\n",
        "            # for i in range(12):\n",
        "            #     if hybrid_action[i*5] == 0 and hybrid_action[i*5 + 1] == 0:\n",
        "            #         hybrid_action[i*5 + 4] += obs1[i]\n",
        "            #     else:\n",
        "            #         hybrid_action[i*5] += obs1[i]\n",
        "            self.obs = hybrid_action\n",
        "            self.get_obs(hybrid_action)\n",
        "\n",
        "            return self.get_obs(hybrid_action) #hybrid_action\n",
        "\n",
        "    def step(self,action):\n",
        "\n",
        "        acts = self.obs\n",
        "\n",
        "        obs0 = []\n",
        "        obs1 = []  # swing 0 stance 1\n",
        "        # print(acts)\n",
        "        for i in range(12):\n",
        "            if acts[i*5] == 0 and acts[i*5 + 1] == 0:\n",
        "                obs0.extend([1])\n",
        "                # obs1.extend([obs[i*5+4]])\n",
        "            else:\n",
        "                obs0.extend([0])\n",
        "                # obs1.extend([obs[i*5]])\n",
        "\n",
        "        penalty_action = 0\n",
        "\n",
        "        for i in range(12):\n",
        "            if obs0[i] == 1:\n",
        "                acts[i*5+4] += action[i]\n",
        "                penalty_action += action[i]*action[i]\n",
        "            else:\n",
        "                acts[i*5] += action[i]\n",
        "                penalty_action += action[i]*action[i]\n",
        "\n",
        "        action = acts\n",
        "        \n",
        "        try:\n",
        "            self.robot.Step(action)\n",
        "        except:\n",
        "            print(action,self.obs)\n",
        "        \n",
        "        \n",
        "        current_time = self.robot.GetTimeSinceReset()\n",
        "        \n",
        "        # done = self._termination()\n",
        "\n",
        "        if self._env_step_counter > 2500 or  self._termination():\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        # done = self.is_fallen()\n",
        "        \n",
        "        self._env_step_counter += 1\n",
        "        reward = self._reward()\n",
        "        if self._termination():\n",
        "            reward -= 10\n",
        "        else:\n",
        "            reward += 0.5\n",
        "\n",
        "        reward -= penalty_action*10\n",
        "        # elif penity_action > 0.01*6:\n",
        "        # else:\n",
        "        #     reward += 0.3\n",
        "\n",
        "\n",
        "        obs = self.mpc_action()\n",
        "        # self.obs = obs \n",
        "        return np.clip(obs,-20,20), reward, done, {}\n",
        "\n",
        "    def render(self, mode=\"rgb_array\", close=False):\n",
        "\n",
        "        cam_dist = 1.0\n",
        "        cam_yaw = 0\n",
        "        cam_pitch = -30\n",
        "\n",
        "        RENDER_HEIGHT = 360\n",
        "        RENDER_WIDTH = 480\n",
        "\n",
        "        gemPos, gemOrn = self.p.getBasePositionAndOrientation(self.robot_uid)\n",
        "        # if mode != \"rgb_array\":\n",
        "        #   self.p.resetDebugVisualizerCamera( cameraDistance=1.5, cameraYaw=-30, cameraPitch=-30, cameraTargetPosition=gemPos)\n",
        "        #   return np.array([])\n",
        "\n",
        "        # gemPos, gemOrn = self.p.getBasePositionAndOrientation(self.robot_uid)\n",
        "\n",
        "        # self.p.resetDebugVisualizerCamera( cameraDistance=1.5, cameraYaw=-30, cameraPitch=-30, cameraTargetPosition=gemPos)\n",
        "\n",
        "\n",
        "        \n",
        "        view_matrix = self.p.computeViewMatrixFromYawPitchRoll(\n",
        "            cameraTargetPosition=gemPos,\n",
        "            distance=cam_dist,\n",
        "            yaw=cam_yaw,\n",
        "            pitch=cam_pitch,\n",
        "            roll=0,\n",
        "            upAxisIndex=2)\n",
        "        proj_matrix = self.p.computeProjectionMatrixFOV(fov=60,\n",
        "                                                                      aspect=float(RENDER_WIDTH) /\n",
        "                                                                      RENDER_HEIGHT,\n",
        "                                                                      nearVal=0.1,\n",
        "                                                                      farVal=100.0)\n",
        "        (_, _, px, _, _) = self.p.getCameraImage(\n",
        "            width=RENDER_WIDTH,\n",
        "            height=RENDER_HEIGHT,\n",
        "            renderer=self.p.ER_BULLET_HARDWARE_OPENGL,\n",
        "            viewMatrix=view_matrix,\n",
        "            projectionMatrix=proj_matrix)\n",
        "        rgb_array = np.array(px)\n",
        "        rgb_array = rgb_array[:, :, :3]\n",
        "        return rgb_array\n",
        "\n",
        "    def _reward(self):\n",
        "        current_base_velocity = self.robot.GetBaseVelocity()\n",
        "        # if self._is_render:\n",
        "        #     desire_base_velocity = self.p.readUserDebugParameter(self.steering1Slider)\n",
        "        # else:\n",
        "        #     desire_base_velocity = 3.6\n",
        "        desire_base_velocity = 3.6\n",
        "        co = abs(current_base_velocity[0]-desire_base_velocity) + 1\n",
        "        reward = (current_base_velocity[0]- 0.0)/(co*10)\n",
        "        reward -= (abs(current_base_velocity[1]) + abs(current_base_velocity[2]))/(co*20)\n",
        "\n",
        "        orientation = self.robot.GetTrueBaseOrientation()\n",
        "        rot_matrix = self.p.getMatrixFromQuaternion(orientation)\n",
        "        local_up_vec = rot_matrix[6:]\n",
        "        shake_reward = -abs(np.dot(np.asarray([1, 1, 0]), np.asarray(local_up_vec)))\n",
        "\n",
        "        reward += shake_reward*0.5\n",
        "        \n",
        "\n",
        "        # if current_base_velocity[0]-desire_base_velocity > 0:\n",
        "        #     reward = 1\n",
        "        # elif abs(current_base_velocity[0]-desire_base_velocity) < 0.5:\n",
        "        #     reward = 0.3\n",
        "        # elif abs(current_base_velocity[0]-desire_base_velocity) < 1:\n",
        "        #     reward = 0.\n",
        "        # elif abs(current_base_velocity[0]-desire_base_velocity) < 1.5:\n",
        "        #     reward = -0.1\n",
        "        # else:\n",
        "        #     reward = -0.3\n",
        "        \n",
        "        \n",
        "\n",
        "        # forward_reward = current_base_position[0] - self._last_base_position[0]\n",
        "        # # Cap the forward reward if a cap is set.\n",
        "        # forward_reward = min(forward_reward, self._forward_reward_cap)\n",
        "        # # Penalty for sideways translation.\n",
        "        # drift_reward = -abs(current_base_position[1] - self._last_base_position[1])\n",
        "        # # Penalty for sideways rotation of the body.\n",
        "        # orientation = self.minitaur.GetBaseOrientation()\n",
        "        # rot_matrix = pybullet.getMatrixFromQuaternion(orientation)\n",
        "        # local_up_vec = rot_matrix[6:]\n",
        "        # shake_reward = -abs(np.dot(np.asarray([1, 1, 0]), np.asarray(local_up_vec)))\n",
        "        # energy_reward = -np.abs(\n",
        "        #     np.dot(self.minitaur.GetMotorTorques(),\n",
        "        #         self.minitaur.GetMotorVelocities())) * self._time_step\n",
        "        # objectives = [forward_reward, energy_reward, drift_reward, shake_reward]\n",
        "        # weighted_objectives = [o * w for o, w in zip(objectives, self._objective_weights)]\n",
        "        # reward = sum(weighted_objectives)\n",
        "        # self._objectives.append(objectives)\n",
        "        return reward\n",
        "\n",
        "# env2 = MyEnv()\n",
        "def make_env(rank, seed=0):\n",
        "    \"\"\"\n",
        "    Utility function for multiprocessed env.\n",
        "    :param env_id: (str) the environment ID\n",
        "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
        "    :param seed: (int) the inital seed for RNG\n",
        "    :param rank: (int) index of the subprocess\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        env = CustomGymEnv()\n",
        "        env.seed(seed + rank)\n",
        "        return env\n",
        "    set_random_seed(seed)\n",
        "    return _init\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_1HF23DQ6br"
      },
      "source": [
        "# run tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cmnvOHcOmC9"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/drive/My Drive/Colab Notebooks/files/tb/mpc/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG7FZ7RRYGC-"
      },
      "source": [
        "# video setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJp1vKzrn9Fh"
      },
      "source": [
        "from typing import Any, Dict\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class VideoRecorderCallback(BaseCallback):\n",
        "    def __init__(self, eval_env: gym.Env, render_freq: int, n_eval_episodes: int = 1, deterministic: bool = True):\n",
        "        \"\"\"\n",
        "        Records a video of an agent's trajectory traversing ``eval_env`` and logs it to TensorBoard\n",
        "\n",
        "        :param eval_env: A gym environment from which the trajectory is recorded\n",
        "        :param render_freq: Render the agent's trajectory every eval_freq call of the callback.\n",
        "        :param n_eval_episodes: Number of episodes to render\n",
        "        :param deterministic: Whether to use deterministic or stochastic policy\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._eval_env = eval_env\n",
        "        self._render_freq = render_freq\n",
        "        self._n_eval_episodes = n_eval_episodes\n",
        "        self._deterministic = deterministic\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self._render_freq == 0:\n",
        "            screens = []\n",
        "\n",
        "            def grab_screens(_locals: Dict[str, Any], _globals: Dict[str, Any]) -> None:\n",
        "                \"\"\"\n",
        "                Renders the environment in its current state, recording the screen in the captured `screens` list\n",
        "\n",
        "                :param _locals: A dictionary containing all local variables of the callback's scope\n",
        "                :param _globals: A dictionary containing all global variables of the callback's scope\n",
        "                \"\"\"\n",
        "                screen = self._eval_env.render(mode=\"rgb_array\")\n",
        "                # PyTorch uses CxHxW vs HxWxC gym (and tensorflow) image convention\n",
        "                screens.append(screen.transpose(2, 0, 1))\n",
        "\n",
        "            evaluate_policy(\n",
        "                self.model,\n",
        "                self._eval_env,\n",
        "                callback=grab_screens,\n",
        "                n_eval_episodes=self._n_eval_episodes,\n",
        "                deterministic=self._deterministic,\n",
        "            )\n",
        "            self.logger.record(\n",
        "                \"trajectory/video\",\n",
        "                Video(th.ByteTensor([screens]), fps=40),\n",
        "                exclude=(\"stdout\", \"log\", \"json\", \"csv\"),\n",
        "            )\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnPD_tbMTr8Z"
      },
      "source": [
        "# train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tYkrkHsRyn_"
      },
      "source": [
        "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from stable_baselines3 import A2C,PPO,SAC\n",
        "\n",
        "from stable_baselines3.common.logger import Video\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "\n",
        "\n",
        "eval_env = CustomGymEnv()\n",
        "eval_env = DummyVecEnv([lambda: eval_env])\n",
        "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False,clip_obs=20., gamma=0.99) # when training norm_reward = True\n",
        "\n",
        "num_cpu = 4\n",
        "env = SubprocVecEnv([make_env(i) for i in range(num_cpu)])\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=False,clip_obs=20., gamma=0.99) # when training norm_reward = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "video_recorder = VideoRecorderCallback(eval_env, render_freq=10000)\n",
        "\n",
        "\n",
        "\n",
        "model_load_name = \"mpc_ppo0410\"\n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/files/{model_load_name}\" \n",
        "model = PPO.load(path, env,  tensorboard_log=\"/content/drive/My Drive/Colab Notebooks/files/tb/mpc/\", verbose=0)\n",
        "\n",
        "import torch as th\n",
        "\n",
        "#### new model\n",
        "# policy_kwargs = dict(activation_fn=th.nn.Tanh,net_arch=[dict(pi=[256, 256], vf=[256, 256])])\n",
        "# model = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs, tensorboard_log=\"/content/drive/My Drive/Colab Notebooks/files/tb/mpc/\", verbose=0)\n",
        "\n",
        "checkpoint_callback = CheckpointCallback(save_freq=50000, save_path='/content/drive/My Drive/Colab Notebooks/files/temp_model_logs/tmp/')\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path='/content/drive/My Drive/Colab Notebooks/files/temp_model_logs/best_model',\n",
        "                            log_path='/content/drive/My Drive/Colab Notebooks/files/temp_model_logs/results', eval_freq=10000)\n",
        "callback = CallbackList([checkpoint_callback, eval_callback, video_recorder])\n",
        "tmsp = 1000*6000\n",
        "model.learn(total_timesteps=tmsp, callback=callback)  # , eval_freq=10, n_eval_episodes=1, eval_log_path=\"/content/drive/My Drive/Colab Notebooks/files/ev/\"\n",
        "\n",
        "model_save_name = \"mpc_ppo0411\"\n",
        "\n",
        "path = F\"/content/drive/My Drive/Colab Notebooks/files/{model_save_name}\"\n",
        "\n",
        "model.save(path)\n",
        "\n",
        "stats_name = \"vn_mpc_ppo03.pkl\"\n",
        "stats_path = F\"/content/drive/My Drive/Colab Notebooks/files/{stats_name}\"\n",
        "env.save(stats_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}